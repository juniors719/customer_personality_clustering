{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96ac22ba",
      "metadata": {
        "id": "96ac22ba"
      },
      "source": [
        "# Trabalho Prático 2 – Aprendizado Não Supervisionado  \n",
        "## Segmentação de Clientes com Algoritmos de Clusterização\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juniors719/customer_personality_clustering/blob/main/customer_segmentation.ipynb)\n",
        "\n",
        "### Equipe:\n",
        "* Francisco Djalma Pereira da Silva Júnior - 554222\n",
        "* Francisco Leudes Bezerra Neto - 552478\n",
        "* Pablo Vinícius da Silva Araújo - 574229\n",
        "\n",
        "### Objetivo:\n",
        "O objetivo principal deste projeto é aplicar, comparar e avaliar o desempenho de três algoritmos de clusterização distintos a um conjunto de dados real. O processo envolve todas as etapas de um projeto de ciência de dados, desde a exploração e pré-processamento dos dados até a aplicação dos modelos e a interpretação dos resultados. Ao final, buscamos identificar segmentos (clusters) de clientes com perfis e comportamentos semelhantes, que possam ser utilizados para direcionar estratégias de marketing mais eficazes.\n",
        "\n",
        "### Descrição do Conjunto de Dados:\n",
        "Para este trabalho, foi selecionado o dataset **\"Customer Personality Analysis\"**, disponível publicamente na plataforma Kaggle.\n",
        "* **Fonte:** [Kaggle - Customer Personality Analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis)\n",
        "* **Amostras:** O dataset original contém **2.240** registros de clientes.\n",
        "* **Características:** Possui **29** atributos (features) que detalham o perfil de cada consumidor em quatro categorias principais:\n",
        "    * **Pessoas:** Dados demográficos como ano de nascimento, educação, estado civil e renda.\n",
        "    * **Produtos:** Gastos em diferentes categorias de produtos (vinhos, carnes, frutas, etc.).\n",
        "    * **Promoções:** Engajamento dos clientes com campanhas de marketing anteriores.\n",
        "    * **Lugar:** Canais de compra utilizados pelos clientes (loja física, site, catálogo).\n",
        "\n",
        "Este conjunto de dados é ideal para a tarefa de clusterização, pois não possui rótulos predefinidos (variável alvo), característica típica de problemas de aprendizado não supervisionado. A riqueza de atributos comportamentais e demográficos torna possível a identificação de segmentos relevantes.\n",
        "\n",
        "### Algoritmos Utilizados\n",
        "\n",
        "- **K-Means:** Rápido e eficiente para grandes datasets; assume clusters esféricos e de tamanho similar.\n",
        "- **Hierárquico (Aglomerativo):** Permite análise mais interpretável via dendrograma; não requer definição prévia do número de clusters.\n",
        "- **DBSCAN:** Identifica clusters de formatos arbitrários e detecta outliers naturalmente.\n",
        "\n",
        "### Pipeline do Projeto\n",
        "1. Importação e visualização inicial dos dados\n",
        "2. Limpeza de dados e engenharia de atributos\n",
        "3. Análise exploratória e identificação de outliers\n",
        "4. Pré-processamento (normalização e codificação)\n",
        "5. Aplicação dos algoritmos de clusterização\n",
        "6. Avaliação e comparação dos modelos\n",
        "7. Visualização com PCA e t-SNE\n",
        "8. Perfilamento e interpretação dos clusters\n",
        "\n",
        "### Divisão de Tarefas:\n",
        "\n",
        "| Etapa                                          | Djalma | Leudes | Pablo |\n",
        "|------------------------------------------------|:------:|:------:|:-----:|\n",
        "| Escolha do dataset e definição do problema     |   ✔    |   ✔    |   ✔   |\n",
        "| Importação e visualização inicial dos dados    |        |   ✔    |       |\n",
        "| Limpeza e engenharia de features               |   ✔    |        |       |\n",
        "| Análise exploratória e remoção de outliers     |        |        |   ✔   |\n",
        "| Pré-processamento (encoding e scaling)         |   ✔    |        |   ✔   |\n",
        "| Modelagem com K-Means                          |   ✔    |        |       |\n",
        "| Modelagem com Clusterização Hierárquica        |        |   ✔    |       |\n",
        "| Modelagem com DBSCAN                           |        |        |   ✔   |\n",
        "| Avaliação dos modelos                          |   ✔    |   ✔    |   ✔   |\n",
        "| Visualização dos clusters (PCA e t-SNE)        |   ✔    |        |       |\n",
        "| Conclusão e perfilamento                       |   ✔    |   ✔    |   ✔   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113480af",
      "metadata": {
        "id": "113480af"
      },
      "source": [
        "# Importação de bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46708c30",
      "metadata": {
        "id": "46708c30"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas para manipulação de dados, matemática e visualização\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importação das ferramentas de Machine Learning do Scikit-learn que serão usadas\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Importação de bibliotecas para ajudar com a visualização e avisos\n",
        "import warnings\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# --- CONFIGURAÇÕES GERAIS ---\n",
        "# Ignorar avisos de bibliotecas para um output mais limpo\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Define um estilo visual padrão para os gráficos\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "import missingno as mn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b870690e",
      "metadata": {
        "id": "b870690e"
      },
      "source": [
        "# Leitura do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073f6519",
      "metadata": {
        "id": "073f6519"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # O arquivo 'marketing_campaign.csv' deve estar na mesma pasta que o seu notebook\n",
        "    url = \"https://raw.githubusercontent.com/juniors719/customer_personality_clustering/refs/heads/main/marketing_campaign.csv\"\n",
        "    df = pd.read_csv(url, sep='\\t')\n",
        "    print(\"--- Dataset Carregado com Sucesso! ---\")\n",
        "    print(f\"O dataset tem {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRO: O arquivo 'marketing_campaign.csv' não foi encontrado.\")\n",
        "    print(\"Por favor, certifique-se de que o arquivo está na mesma pasta que o seu notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482818d2",
      "metadata": {
        "id": "482818d2"
      },
      "source": [
        "# Primeira análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229230e9",
      "metadata": {
        "id": "229230e9"
      },
      "outputs": [],
      "source": [
        "# 1. Visualização das primeiras linhas do dataset\n",
        "print(\"\\n--- Visualizando as 5 primeiras linhas do dataset: ---\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a041aed",
      "metadata": {
        "id": "7a041aed"
      },
      "source": [
        "### Verificação de Tipos de Dados e Contagem de Nulos\n",
        "\n",
        "O `.info()` nos dá uma visão rápida sobre os tipos de dados e a presença de valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3624acfa",
      "metadata": {
        "id": "3624acfa"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dba955",
      "metadata": {
        "id": "56dba955"
      },
      "source": [
        "### Estatísticas Descritivas (Numéricas)\n",
        "\n",
        "O `.describe()` resume as principais métricas estatísticas das colunas numéricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4613d7b7",
      "metadata": {
        "id": "4613d7b7"
      },
      "outputs": [],
      "source": [
        "display(df.describe().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d851bc9b",
      "metadata": {
        "id": "d851bc9b"
      },
      "source": [
        "### Contagem de Valores Nulos por Coluna\n",
        "\n",
        "A presença de dados ausentes é um dos problemas mais comuns em projetos de ciência de dados. É crucial identificá-los e tratá-los corretamente, pois a maioria dos algoritmos de Machine Learning não funciona com valores nulos.\n",
        "\n",
        "**O Impacto dos Dados Ausentes:**\n",
        "\n",
        "- **Redução do Poder Estatístico:** A ausência de dados diminui o tamanho da amostra, o que pode enfraquecer os resultados de testes estatísticos e a confiança nos modelos.\n",
        "- **Viés nos Resultados:** Se os dados não estiverem faltando de forma completamente aleatória, a sua ausência pode introduzir um viés significativo. Por exemplo, se pessoas com renda mais baixa tendem a não informar sua renda, qualquer análise sobre o impacto da renda no consumo será distorcida.\n",
        "- **Problemas Técnicos:** Como mencionado, algoritmos como k-Means param ou geram erros ao encontrar valores nulos.\n",
        "\n",
        "Vamos agora visualizar detalhadamente onde estão os nossos dados ausentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afada73d",
      "metadata": {
        "id": "afada73d"
      },
      "outputs": [],
      "source": [
        "def missing_data(df):\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\n",
        "    percentage = (df.isnull().sum() / df.isnull().count() * 100).sort_values(ascending=False)\n",
        "    return pd.concat([total, percentage], axis=1, keys=['Total', 'Porcentagem'])\n",
        "\n",
        "print(\"\\n\\n--- Análise de Dados Ausentes por Coluna ---\")\n",
        "missing_values = missing_data(df)\n",
        "display(missing_values.style.background_gradient(cmap='Reds'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999408db",
      "metadata": {
        "id": "999408db"
      },
      "outputs": [],
      "source": [
        "# Visualizar a matriz de valores ausentes\n",
        "print(\"\\n\\n--- Matriz de Valores Ausentes ---\")\n",
        "mn.matrix(df,color = (0,0,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50ef9",
      "metadata": {
        "id": "3fd50ef9"
      },
      "source": [
        "###  Verificação de Linhas Duplicadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be97b380",
      "metadata": {
        "id": "be97b380"
      },
      "outputs": [],
      "source": [
        "num_duplicados = df.duplicated().sum()\n",
        "print(f\"Número de linhas duplicadas encontradas: {num_duplicados}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60249a1b",
      "metadata": {
        "id": "60249a1b"
      },
      "source": [
        "### Contagem de Valores Únicos (Cardinalidade) por Coluna\n",
        "\n",
        "O `.nunique()` é essencial para identificar colunas constantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090e69a8",
      "metadata": {
        "id": "090e69a8"
      },
      "outputs": [],
      "source": [
        "display(df.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796d7421",
      "metadata": {
        "id": "796d7421"
      },
      "source": [
        "A análise de cardinalidade revelou que as colunas `Z_CostContact` e `Z_Revenue` possuem apenas **1 valor único**. Elas são constantes e serão removidas na etapa de limpeza por não adicionarem informação útil."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33c8eb4",
      "metadata": {
        "id": "d33c8eb4"
      },
      "source": [
        "### Conclusão da Análise Diagnóstica\n",
        "\n",
        "A partir da análise diagnóstica, podemos extrair as seguintes conclusões que guiarão nosso próximo passo:\n",
        "\n",
        "* **Estrutura Confirmada:** O dataset possui **2240 linhas** e **29 colunas**.\n",
        "* **Dados Ausentes Identificados:** O diagnóstico confirma que a única coluna com valores faltantes é a `Income`, com **24 registros nulos**.\n",
        "* **Sem Duplicatas:** Não há linhas duplicadas no dataset.\n",
        "* **Colunas Constantes:** As colunas `Z_CostContact` e `Z_Revenue` possuem apenas um valor único, indicando que não contribuem com informação útil para a clusterização. Elas serão removidas na etapa de limpeza.\n",
        "* **Necessidade de Pré-processamento:** O dataset contém uma mistura de tipos de dados (numéricos e de texto) que exigirá tratamento (normalização e codificação) antes da modelagem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6ffa4c",
      "metadata": {
        "id": "df6ffa4c"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fca1f3",
      "metadata": {
        "id": "08fca1f3"
      },
      "outputs": [],
      "source": [
        "df_clean = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56ac4b3",
      "metadata": {
        "id": "e56ac4b3"
      },
      "source": [
        "### Preenchimento de Dados Ausentes\n",
        "\n",
        "Para lidar com os dados ausentes, uma abordagem comum é preencher os valores nulos com a mediana da coluna correspondente. Isso é especialmente útil para colunas numéricas, como a `Income`, onde a média pode fornecer uma estimativa razoável do valor ausente sem introduzir viés significativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e57231",
      "metadata": {
        "id": "57e57231"
      },
      "outputs": [],
      "source": [
        "income_median = df_clean['Income'].median()\n",
        "df_clean['Income'].fillna(income_median, inplace=True)\n",
        "print(f\"Valores nulos em 'Income' após imputação: {df_clean['Income'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a587008a",
      "metadata": {
        "id": "a587008a"
      },
      "source": [
        "### Engenharia de Features\n",
        "A engenharia de features é uma etapa crucial no pré-processamento de dados, onde transformamos e criamos novas variáveis a partir das existentes para melhorar o desempenho dos modelos de Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca5ca82",
      "metadata": {
        "id": "4ca5ca82"
      },
      "source": [
        "* **`Idade`**: É uma variável demográfica muito mais intuitiva e diretamente interpretável do que o ano de nascimento.\n",
        "* **`Gasto_Total`**: Consolida todos os gastos em uma única métrica poderosa que representa o valor total de compra de cada cliente.\n",
        "* **`Tempo_de_Cliente_dias`**: Mede a longevidade do relacionamento do cliente com a empresa, um excelente indicador de lealdade.\n",
        "* **`Total_Filhos`** e **`Possui_Filhos`**: Simplificam a estrutura familiar, permitindo analisar de forma direta e eficaz as diferenças de comportamento entre clientes que são pais e os que não são."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfc6b01",
      "metadata": {
        "id": "dbfc6b01"
      },
      "outputs": [],
      "source": [
        "# CRIAÇÃO DE FEATURES\n",
        "\n",
        "# Criação da Feature 'Age' a partir do ano de nascimento\n",
        "df_clean['Age'] = 2025 - df_clean['Year_Birth']\n",
        "\n",
        "# Criação da Feature 'Total_Spent' que soma os gastos em diferentes categorias\n",
        "df_clean['Total_Spent'] = df_clean[[col for col in df.columns if 'Mnt' in col]].sum(axis=1)\n",
        "\n",
        "# Conversão da coluna 'Dt_Customer' para o formato datetime\n",
        "df_clean['Dt_Customer'] = pd.to_datetime(df_clean['Dt_Customer'], dayfirst=True)\n",
        "\n",
        "# Criação da Feature 'Customer_Tenure' que calcula o tempo de relacionamento com o cliente\n",
        "df_clean['Customer_Tenure'] = (pd.to_datetime('2025-07-06') - df_clean['Dt_Customer']).dt.days # Usando a data da entrega\n",
        "\n",
        "# Criação da Feature 'Children' que soma as colunas 'Kidhome' e 'Teenhome'\n",
        "df_clean['Children'] = df_clean['Kidhome'] + df_clean['Teenhome']\n",
        "\n",
        "# Criação da Feature 'Is_Parent' que indica se o cliente é pai/mãe\n",
        "df_clean['Is_Parent'] = np.where(df_clean['Children'] > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836837b8",
      "metadata": {
        "id": "836837b8"
      },
      "source": [
        "Para otimizar nosso dataset, vamos agora simplificar algumas colunas categóricas. O objetivo é agrupar categorias que têm um significado semelhante ou que são muito raras, reduzindo o \"ruído\" nos dados e tornando os padrões mais claros para os algoritmos.\n",
        "\n",
        "* **`Estado_Civil`**: As diversas categorias serão agrupadas em duas classes principais: `In_Relationship` (para `Married` e `Together`) e `Single` (para as demais). Esta simplificação captura a principal informação sobre a estrutura do domicílio do cliente (vive com um parceiro ou não).\n",
        "* **`Escolaridade`**: Os diferentes níveis de educação serão consolidados em três grupos mais amplos e significativos: `Undergraduate`, `Graduate` e `Postgraduate`.\n",
        "* **`Tamanho_Familia`**: Por fim, criaremos uma nova feature combinando o `Estado_Civil` simplificado com o número de filhos (`Total_Filhos`), resultando em uma única e poderosa variável que descreve a composição do lar do cliente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8069b35",
      "metadata": {
        "id": "b8069b35"
      },
      "outputs": [],
      "source": [
        "# SIMPLIFICAÇÃO DE FEATURES\n",
        "\n",
        "# Simplificação da coluna 'Marital_Status', a fim de agrupar estados civis semelhantes.\n",
        "df_clean['Marital_Status'] = df_clean['Marital_Status'].replace({\n",
        "    'Married': 'In_Relationship',\n",
        "    'Together': 'In_Relationship',\n",
        "    'Single': 'Single',\n",
        "    'Divorced': 'Single',\n",
        "    'Widow': 'Single',\n",
        "    'Alone': 'Single',\n",
        "    'Absurd': 'Single',\n",
        "    'YOLO': 'Single'\n",
        "})\n",
        "\n",
        "# Simplificação da coluna 'Education', agrupando níveis de escolaridade semelhantes.\n",
        "df_clean['Education'] = df_clean['Education'].replace({\n",
        "    'Basic': 'Undergraduate', '2n Cycle': 'Undergraduate', 'Graduation': 'Graduate',\n",
        "    'Master': 'Postgraduate', 'PhD': 'Postgraduate'\n",
        "})\n",
        "\n",
        "# Criação de Feature Combinada\n",
        "df_clean['Family_Size'] = df_clean['Marital_Status'].map({'Single': 1, 'In_Relationship': 2}) + df_clean['Children']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebba5f75",
      "metadata": {
        "id": "ebba5f75"
      },
      "source": [
        "Após a criação de novas features mais informativas, algumas das colunas originais se tornaram redundantes ou são irrelevantes para a modelagem. Para criar um dataset final mais limpo e focado, vamos remover os seguintes grupos de colunas:\n",
        "\n",
        "* **Identificadores:** A coluna `ID` é apenas um identificador único de cliente e não contém informação sobre seu comportamento.\n",
        "* **Features Redundantes:** As colunas `Year_Birth` e `Dt_Customer` já foram utilizadas para criar `Idade` e `Tempo_de_Cliente_dias`, respectivamente. Da mesma forma, `Kidhome` e `Teenhome` foram consolidadas na feature `Total_Filhos`. Manter as originais apenas adicionaria informação repetida.\n",
        "* **Features Constantes:** As colunas `Z_CostContact` e `Z_Revenue` foram identificadas na análise diagnóstica como constantes (possuem o mesmo valor para todos os clientes) e, portanto, não têm poder preditivo.\n",
        "\n",
        "A remoção dessas colunas resultará em um dataset mais enxuto e eficiente para a etapa de clusterização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326c8745",
      "metadata": {
        "id": "326c8745"
      },
      "outputs": [],
      "source": [
        "# REMOÇÃO DE FEATURES INÚTEIS\n",
        "\n",
        "to_drop = ['ID', 'Year_Birth', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue', 'Kidhome', 'Teenhome']\n",
        "df_clean.drop(columns=to_drop, inplace=True)\n",
        "print(f\"\\nColunas removidas do dataset: {to_drop}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945d3b9a",
      "metadata": {
        "id": "945d3b9a"
      },
      "source": [
        "Para facilitar a leitura, a interpretação dos gráficos e a análise dos resultados, todas as colunas do dataframe `df_clean` foram traduzidas para o português. Esta etapa melhora a clareza do notebook e torna a comunicação dos insights mais direta e intuitiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c562080b",
      "metadata": {
        "id": "c562080b"
      },
      "outputs": [],
      "source": [
        "# --- TRADUÇÃO DAS COLUNAS PARA PORTUGUÊS ---\n",
        "\n",
        "# Dicionário mapeando os nomes originais para os nomes em português\n",
        "mapeamento_nomes = {\n",
        "    'Education': 'Escolaridade',\n",
        "    'Marital_Status': 'Estado_Civil',\n",
        "    'Income': 'Renda_Anual',\n",
        "    'Recency': 'Dias_Ultima_Compra',\n",
        "    'MntWines': 'Gasto_Vinhos',\n",
        "    'MntFruits': 'Gasto_Frutas',\n",
        "    'MntMeatProducts': 'Gasto_Carnes',\n",
        "    'MntFishProducts': 'Gasto_Peixes',\n",
        "    'MntSweetProducts': 'Gasto_Doces',\n",
        "    'MntGoldProds': 'Gasto_Ouro',\n",
        "    'NumDealsPurchases': 'Compras_com_Desconto',\n",
        "    'NumWebPurchases': 'Compras_pela_Web',\n",
        "    'NumCatalogPurchases': 'Compras_por_Catalogo',\n",
        "    'NumStorePurchases': 'Compras_na_Loja',\n",
        "    'NumWebVisitsMonth': 'Visitas_no_Site_Mes',\n",
        "    'AcceptedCmp3': 'Aceitou_Campanha_3',\n",
        "    'AcceptedCmp4': 'Aceitou_Campanha_4',\n",
        "    'AcceptedCmp5': 'Aceitou_Campanha_5',\n",
        "    'AcceptedCmp1': 'Aceitou_Campanha_1',\n",
        "    'AcceptedCmp2': 'Aceitou_Campanha_2',\n",
        "    'Complain': 'Reclamou_2_Anos',\n",
        "    'Response': 'Aceitou_Ultima_Campanha',\n",
        "    'Age': 'Idade',\n",
        "    'Total_Spent': 'Gasto_Total',\n",
        "    'Customer_Tenure': 'Tempo_de_Cliente_dias',\n",
        "    'Children': 'Total_Filhos',\n",
        "    'Is_Parent': 'Possui_Filhos',\n",
        "    'Family_Size': 'Tamanho_Familia'\n",
        "}\n",
        "\n",
        "df_clean.rename(columns=mapeamento_nomes, inplace=True)\n",
        "\n",
        "print(\"\\n--- Colunas traduzidas para o português ---\")\n",
        "print(\"Visualizando o dataframe com os novos nomes de colunas:\")\n",
        "display(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ba860e",
      "metadata": {
        "id": "59ba860e"
      },
      "source": [
        "# Análise Exploratória"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decb43fd",
      "metadata": {
        "id": "decb43fd"
      },
      "source": [
        "Agora que nossos dados estão limpos e enriquecidos, vamos iniciar a análise exploratória visual. Começaremos com uma **análise univariada**, ou seja, analisando uma variável de cada vez para entender suas características individuais.\n",
        "\n",
        "### Análise Univariada\n",
        "\n",
        "Usaremos **histogramas** para visualizar a distribuição de duas das nossas features mais importantes: `Idade` e `Gasto_Total`.\n",
        "\n",
        "* **Histograma de Idade:** Nos ajudará a entender a faixa etária predominante dos nossos clientes. Existem mais clientes jovens, de meia-idade ou mais velhos? A distribuição é uniforme ou concentrada em algum grupo?\n",
        "* **Histograma de Gasto Total:** Revelará a distribuição do poder de compra. É provável que encontremos uma **assimetria à direita**, onde a maioria dos clientes tem um gasto moderado e uma minoria de \"high-rollers\" gasta valores muito mais altos.\n",
        "\n",
        "Compreender essas distribuições é o primeiro passo para identificar os diferentes perfis de clientes que compõem nossa base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eef96f6",
      "metadata": {
        "id": "2eef96f6"
      },
      "outputs": [],
      "source": [
        "# Verificar distribuição de idade\n",
        "sns.histplot(df_clean['Idade'], bins=20, kde=True)\n",
        "plt.title('Distribuição de Idade dos Clientes')\n",
        "plt.show()\n",
        "\n",
        "# Gasto Total\n",
        "sns.histplot(df_clean['Gasto_Total'], bins=30, kde=True)\n",
        "plt.title('Distribuição do Gasto Total')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4418926d",
      "metadata": {
        "id": "4418926d"
      },
      "source": [
        "### Análise Visual para Identificação de Outliers\n",
        "\n",
        "Para aprofundar nossa análise e preparar os dados para a clusterização, é essencial identificar a presença de **outliers**. Outliers são pontos de dados que se desviam significativamente do resto do conjunto e podem distorcer os resultados de algoritmos sensíveis à distância, como o k-Means.\n",
        "\n",
        "A ferramenta visual mais eficaz para esta tarefa é o **Boxplot**. Ele nos permite visualizar:\n",
        "* A **mediana** (a linha central).\n",
        "* O **intervalo interquartil (IQR)** (a \"caixa\", que contém 50% dos dados centrais).\n",
        "* Os **\"bigodes\" (whiskers)**, que representam o alcance esperado dos dados.\n",
        "* E, mais importante, os **pontos individuais fora dos bigodes**, que são os nossos potenciais outliers.\n",
        "\n",
        "Vamos gerar boxplots para as nossas principais variáveis contínuas: `Renda_Anual`, `Idade` e `Gasto_Total`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea1232b",
      "metadata": {
        "id": "dea1232b"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
        "fig.suptitle('Análise de Distribuição e Outliers em Features-Chave', fontsize=18)\n",
        "\n",
        "# Usaremos a paleta de cores que definimos no início\n",
        "PALETTE = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\"]\n",
        "\n",
        "# Gráfico 1: Boxplot da Renda Anual\n",
        "sns.boxplot(ax=axes[0], data=df_clean, y='Renda_Anual', color=PALETTE[0])\n",
        "axes[0].set_title('Distribuição da Renda Anual', fontsize=14)\n",
        "axes[0].set_ylabel('Renda Anual (em $)')\n",
        "\n",
        "# Gráfico 2: Boxplot da Idade\n",
        "sns.boxplot(ax=axes[1], data=df_clean, y='Idade', color=PALETTE[1])\n",
        "axes[1].set_title('Distribuição da Idade', fontsize=14)\n",
        "axes[1].set_ylabel('Idade (em anos)')\n",
        "\n",
        "# Gráfico 3: Boxplot do Gasto Total\n",
        "sns.boxplot(ax=axes[2], data=df_clean, y='Gasto_Total', color=PALETTE[2])\n",
        "axes[2].set_title('Distribuição do Gasto Total', fontsize=14)\n",
        "axes[2].set_ylabel('Gasto Total (em $)')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418b081c",
      "metadata": {
        "id": "418b081c"
      },
      "source": [
        "### Remoção de Outliers\n",
        "\n",
        "A análise visual da etapa anterior, especialmente através dos boxplots, confirmou a presença de valores extremos (outliers) nas colunas `Renda_Anual` e `Idade`. Estes pontos, embora poucos, podem distorcer significativamente o cálculo das distâncias e a formação dos clusters, levando a um modelo menos preciso e representativo da maioria dos clientes.\n",
        "\n",
        "Para mitigar esse risco, vamos remover esses registros do nosso dataset. Adotaremos os seguintes critérios de corte:\n",
        "* **Renda Anual:** Manteremos apenas clientes com renda inferior a 200.000.\n",
        "* **Idade:** Manteremos apenas clientes com idade inferior a 90 anos.\n",
        "\n",
        "Esta limpeza resultará em um conjunto de dados mais robusto e preparado para a etapa de modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89dc2a6",
      "metadata": {
        "id": "b89dc2a6"
      },
      "outputs": [],
      "source": [
        "# --- REMOÇÃO DE OUTLIERS ---\n",
        "\n",
        "# Guardamos o número de linhas antes da remoção para verificar o impacto\n",
        "linhas_antes = df_clean.shape[0]\n",
        "\n",
        "# Filtramos o dataframe para manter apenas os clientes com Renda_Anual < 200.000 e Idade < 90\n",
        "# Usamos .copy() para garantir que estamos criando um novo dataframe independente\n",
        "df_sem_outliers = df_clean[(df_clean['Renda_Anual'] < 200000) & (df_clean['Idade'] < 90)].copy()\n",
        "\n",
        "linhas_depois = df_sem_outliers.shape[0]\n",
        "outliers_removidos = linhas_antes - linhas_depois\n",
        "\n",
        "print(\"--- Remoção de Outliers Concluída ---\")\n",
        "print(f\"Número de linhas original: {linhas_antes}\")\n",
        "print(f\"Número de linhas após remover outliers: {linhas_depois}\")\n",
        "print(f\"Total de outliers removidos: {outliers_removidos}\")\n",
        "\n",
        "# O nosso novo dataframe de trabalho agora é o 'df_sem_outliers'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5a4c64",
      "metadata": {
        "id": "8f5a4c64"
      },
      "source": [
        "### Análise de Correlação entre Features-Chave\n",
        "\n",
        "Para finalizar nossa análise exploratória, vamos investigar a **correlação linear** entre as principais variáveis numéricas do nosso conjunto de dados. A correlação mede a força e a direção da relação entre duas variáveis.\n",
        "\n",
        "Usaremos um **mapa de calor (heatmap)** para visualizar a matriz de correlação. Esta é uma forma gráfica e intuitiva de identificar rapidamente quais variáveis se movem juntas.\n",
        "\n",
        "**Como interpretar o mapa de calor:**\n",
        "* **Cores Quentes (próximas ao vermelho - valor próximo de +1):** Indicam uma **correlação positiva forte**. Quando uma variável aumenta, a outra tende a aumentar também. Esperamos ver isso entre `Renda_Anual` e `Gasto_Total`.\n",
        "* **Cores Frias (próximas ao azul - valor próximo de -1):** Indicam uma **correlação negativa forte**. Quando uma variável aumenta, a outra tende a diminuir.\n",
        "* **Cores Neutras (próximas ao branco/cinza - valor próximo de 0):** Indicam pouca ou nenhuma correlação linear.\n",
        "\n",
        "Esta análise nos ajudará a entender as relações subjacentes no comportamento dos clientes antes de aplicarmos os algoritmos de clusterização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a555e467",
      "metadata": {
        "id": "a555e467"
      },
      "outputs": [],
      "source": [
        "# --- MAPA DE CALOR DE CORRELAÇÃO ---\n",
        "\n",
        "# Selecionando as colunas numéricas mais relevantes para a correlação\n",
        "# Usamos o dataframe já sem outliers e com colunas em português\n",
        "cols_para_corr = [\n",
        "    'Renda_Anual', 'Dias_Ultima_Compra', 'Gasto_Total',\n",
        "    'Tempo_de_Cliente_dias', 'Idade', 'Tamanho_Familia', 'Total_Filhos'\n",
        "]\n",
        "\n",
        "# Calculando a matriz de correlação\n",
        "matriz_corr = df_sem_outliers[cols_para_corr].corr()\n",
        "\n",
        "# Criando o mapa de calor\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(\n",
        "    matriz_corr,\n",
        "    annot=True,     # Mostra os valores de correlação no gráfico\n",
        "    fmt=\".2f\",      # Formata os números para duas casas decimais\n",
        "    cmap='coolwarm' # Paleta de cores: quente para positivo, frio para negativo\n",
        ")\n",
        "plt.title('Mapa de Calor de Correlação entre Features-Chave', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f50ec903",
      "metadata": {
        "id": "f50ec903"
      },
      "source": [
        "* **Correlação Positiva Forte (Renda vs. Gasto):** A observação mais clara é a forte correlação positiva (aproximadamente **+0.79**) entre `Renda_Anual` e `Gasto_Total`. Isso confirma a hipótese intuitiva de que clientes com maior renda tendem a gastar mais. Esta será, provavelmente, uma das variáveis mais importantes para a segmentação dos nossos clientes.\n",
        "\n",
        "* **Correlação Negativa (Filhos vs. Gasto):** Existe uma correlação negativa moderada (aproximadamente **-0.50**) entre `Total_Filhos` e `Gasto_Total`. Isso sugere que, em geral, clientes com mais filhos tendem a ter um gasto total menor nos produtos da nossa loja, possivelmente por terem outras prioridades financeiras.\n",
        "\n",
        "* **Correlação Moderada (Idade e Tempo de Cliente):** A `Idade` e o `Tempo_de_Cliente_dias` também mostram uma correlação positiva com o `Gasto_Total`. Isso indica que clientes mais velhos e mais leais (com mais tempo de casa) tendem a gastar mais.\n",
        "\n",
        "* **Pouca Correlação (Recência):** A variável `Dias_Ultima_Compra` (Recência) parece ter uma correlação muito fraca com as outras variáveis, sugerindo que o tempo desde a última compra não está fortemente ligado de forma linear à renda ou ao gasto total nesta visão geral."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd0f63a",
      "metadata": {
        "id": "ccd0f63a"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4814c7",
      "metadata": {
        "id": "2e4814c7"
      },
      "source": [
        "### Label Encoding\n",
        "Converteremos as variáveis categóricas remanescentes (`Escolaridade` e `Estado_Civil`) em representações numéricas, pois os algoritmos de clusterização não processam texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b51185",
      "metadata": {
        "id": "87b51185"
      },
      "outputs": [],
      "source": [
        "# --- PASSO 4: EXECUÇÃO DO ENCODING E SCALING ---\n",
        "\n",
        "# Criamos uma cópia do dataframe já limpo para esta etapa final\n",
        "df_processed = df_sem_outliers.copy()\n",
        "\n",
        "# 1. ENCODING DE VARIÁVEIS CATEGÓRICAS\n",
        "# Seleciona as colunas de texto (tipo 'object')\n",
        "categorical_cols = df_processed.select_dtypes(include='object').columns\n",
        "\n",
        "print(\"--- Codificando as seguintes colunas categóricas: ---\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Aplica o LabelEncoder para transformar texto em números\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df_processed[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb3bcd3",
      "metadata": {
        "id": "dfb3bcd3"
      },
      "source": [
        "### Normalização\n",
        "\n",
        "Padronizar todas as colunas para que tenham média 0 e desvio padrão 1. Isso evita que variáveis com valores grandes (como Renda_Anual) dominem o processo de clusterização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c1acfa",
      "metadata": {
        "id": "c2c1acfa"
      },
      "outputs": [],
      "source": [
        "# 2. NORMALIZAÇÃO (STANDARD SCALING)\n",
        "# Instancia o Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Treina o scaler e transforma o dataframe\n",
        "scaled_features = scaler.fit_transform(df_processed)\n",
        "\n",
        "# Converte o array de volta para um dataframe do pandas com os nomes das colunas\n",
        "# Este é o nosso dataframe final e pronto para a modelagem!\n",
        "df_ready = pd.DataFrame(scaled_features, columns=df_processed.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdcb312",
      "metadata": {
        "id": "1bdcb312"
      },
      "outputs": [],
      "source": [
        "# --- RESULTADO DA ETAPA ---\n",
        "print(\"\\n--- Dataset 100% pronto para a clusterização ---\")\n",
        "print(\"Todas as features são numéricas e estão na mesma escala.\")\n",
        "display(df_ready.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69c18c7",
      "metadata": {
        "id": "a69c18c7"
      },
      "outputs": [],
      "source": [
        "# --- VERIFICAÇÃO DO STANDARDSCALER ---\n",
        "\n",
        "# Calculando a média e o desvio padrão de cada coluna no dataframe normalizado\n",
        "means = df_ready.mean()\n",
        "stds = df_ready.std()\n",
        "\n",
        "# Criando a figura para os gráficos de verificação\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "fig.suptitle('Verificação da Normalização (StandardScaler)', fontsize=18)\n",
        "\n",
        "# Gráfico 1: Média das Features\n",
        "sns.barplot(ax=axes[0], x=means.index, y=means.values, palette=PALETTE)\n",
        "axes[0].set_title('Média de Cada Feature (deve ser ≈ 0)', fontsize=14)\n",
        "axes[0].set_ylabel('Média')\n",
        "axes[0].tick_params(axis='x', rotation=90) # Rotaciona os nomes das colunas para não sobrepor\n",
        "\n",
        "# Gráfico 2: Desvio Padrão das Features\n",
        "sns.barplot(ax=axes[1], x=stds.index, y=stds.values, palette=PALETTE)\n",
        "axes[1].set_title('Desvio Padrão de Cada Feature (deve ser ≈ 1)', fontsize=14)\n",
        "axes[1].set_ylabel('Desvio Padrão')\n",
        "axes[1].tick_params(axis='x', rotation=90)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Exibindo os valores exatos para confirmação numérica\n",
        "print(\"\\n--- Média de cada feature (arredondada para 5 casas decimais): ---\")\n",
        "print(round(means, 5))\n",
        "print(\"\\n--- Desvio Padrão de cada feature (arredondado para 5 casas decimais): ---\")\n",
        "print(round(stds, 5))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}